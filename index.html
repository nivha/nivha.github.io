<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Niv Haim</title>
  
  <meta name="author" content="Niv Haim's Homepage">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-MVYYQHNEK7"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-MVYYQHNEK7');
	</script>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/favicon.png">
  
  <script>
	const copyToClipboard = str => {
	  const el = document.createElement('textarea');
	  el.value = str;
	  document.body.appendChild(el);
	  el.select();
	  document.execCommand('copy');
	  document.body.removeChild(el);
	  alert("The following bibtextex has been copied to clipboard:\n\n" + el.value);
	};
  </script>

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center"><name>Niv Haim</name></p>
              <p>I am a Phd student at the <a href="https://www.weizmann.ac.il/pages/">Weizmann Institute of Science</a>, advised by <a href="https://www.weizmann.ac.il/math/irani/">Prof. Michal Irani</a>.
                I work on computer vision and machine learning.
			  </p>
			  <p> I did my Msc. in theoretical astrophysics, advised by <a href="https://www.weizmann.ac.il/particle/katz/">Prof. Boaz Katz</a>, 
					and worked with <a href="http://www.wisdom.weizmann.ac.il/~ylipman/">Prof. Yaron Lipman</a> on Geometric Deep Learning. 
					I received my BSc. in computer science and physics from the <a href="https://www.technion.ac.il/en/home-2/">Technion</a> (<a href="https://lapidim.cs.technion.ac.il/">Lapidim excellence program</a> alumnus)
			  </p>
              <p style="text-align:center">
                <a href="mailto:nivhaa@gmail.com">Email</a> &nbsp/&nbsp
                <!-- <a href="data/cv.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.co.il/citations?user=f7SCiakAAAAJ">Google Scholar</a> &nbsp/&nbsp
				<a href="https://github.com/nivha">GitHub</a> &nbsp/&nbsp
				<a href="https://linkedin.com/in/niv-haim-736b3b5b">LinkedIn</a> &nbsp/&nbsp
                <a href="https://twitter.com/HaimNiv">Twitter</a>			
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/me.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
		
		
		<hr/>
		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



          <!-- <tr onmouseout="vgpnn_stop()" onmouseover="vgpnn_start()"> -->
		  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='vgpnn_vid'> -->
					<!-- <video  width=100% height=100% muted autoplay loop> -->
						<!-- <source src="images/vgpnn.mp4" style="background-color: white;" type="video/mp4"> -->
						<!-- Your browser does not support the video tag. -->
					<!-- </video> -->
				<!-- </div> -->
                <img src='images/vgpnn.png' width="160" id="vgpnn_image">
              </div>
              <script type="text/javascript">
			  var bibtex_vgpnn = ""
                <!-- function vgpnn_start() { -->
                  <!-- document.getElementById('vgpnn_image').style.display = 'none'; -->
				  <!-- document.getElementById('vgpnn_vid').style.display = 'inline'; -->
                <!-- } -->
                <!-- function vgpnn_stop() { -->
                  <!-- document.getElementById('vgpnn_image').style.display = 'inline'; -->
				  <!-- document.getElementById('vgpnn_vid').style.display = 'none'; -->
                <!-- } -->
                <!-- vgpnn_stop() -->
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
              <papertitle>Diverse Generation from a Single Video Made Possible</papertitle>
              </a>
              <br>
				<strong>Niv Haim*</strong>,
				<a href="https://www.semanticscholar.org/author/Ben-Feinstein/4478112">Ben Finestein*</a>,
				<a href="https://scholar.google.com/citations?user=MaDeUsoAAAAJ">Niv Granot</a>,
				<a href="http://www.wisdom.weizmann.ac.il/~/assafsho/">Assaf Shocher</a>,
				<a href="https://www.weizmann.ac.il/math/bagon/home">Shai Bagon</a>,
				<a href="https://www.weizmann.ac.il/math/dekel/">Tali Dekel</a>,
				<a href="https://www.weizmann.ac.il/math/irani/">Michal Irani</a>
              <br>
              <em>Technical Report</em>, 2021
              <br>
				<div class="btn-group">
				<div class="tooltip"><button>Abstract</button><span class="tooltiptext">Most advanced video generation and manipulation methods train on a large collection of videos. As such, they are restricted to the types of video dynamics they train on. To overcome this limitation, GANs trained on a single video were recently proposed. While these provide more flexibility to a wide variety of video dynamics, they require days to train on a single tiny input video, rendering them impractical. In this paper we present a fast and practical method for video generation and manipulation from a single natural video, which generates diverse high-quality video outputs within seconds (for benchmark videos). Our method can be further applied to full-HD video clips within minutes. Our approach is inspired by a recent advanced patch-nearest-neighbor based approach [Granot et al., 2021], which was shown to significantly outperform single-image GANs, both in run-time and in visual quality. Here we generalize this approach from images to videos, by casting classical space-time patch-based methods as a new generative video model. We adapt the generative image patch nearest neighbor approach to efficiently cope with the huge number of space-time patches in a single video. Our method generates more realistic and higher quality results than single-video GANs (confirmed by quantitative and qualitative evaluations). Moreover, it is disproportionally faster (runtime reduced from several days to seconds). Other than diverse video generation, we demonstrate several other challenging video applications, including spatio-temporal video retargeting (e.g., video extension & video summarization), video structural analogies and conditional video-inpainting.</span></div>
				<a href="http://arxiv.org/abs/2109.08591"><button>ArXiv</button></a>
				<a href="https://nivha.github.io/vgpnn/"><button>Project Page</button></a>
				<!-- <button onclick=copyToClipboard(bibtex_vgpnn)>bibtex</button> -->
				</div>
              <p></p>
            </td>
		</tr>


          <tr onmouseout="cc_stop()" onmouseover="cc_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cc_vid'>
					<video  width=100% height=100% muted autoplay loop>
						<source src="images/cc.mp4" style="background-color: white;" type="video/mp4">
						Your browser does not support the video tag.
					</video>
				</div>
                <img src='images/cc.png' width="160" id="cc_image">
              </div>
              <script type="text/javascript">
			  var bibtex_cc = "@InProceedings{Shocher_2020_arxiv_cc,\nauthor = {Shocher, Assaf and Feinstein, Ben and Haim, Niv and Irani, Michal},\ntitle = {From Discrete to Continuous Convolution Layers},\nbooktitle = arXiv,\nyear = {2020}\n}"
                function cc_start() {
                  document.getElementById('cc_image').style.display = 'none';
				  document.getElementById('cc_vid').style.display = 'inline';
                }
                function cc_stop() {
                  document.getElementById('cc_image').style.display = 'inline';
				  document.getElementById('cc_vid').style.display = 'none';
                }
                cc_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2006.11120">
              <papertitle>From Discrete to Continuous Convolution Layers</papertitle>
              </a>
              <br>
				<a href="http://www.wisdom.weizmann.ac.il/~/assafsho/">Assaf Shocher*</a>,
				<a href="https://www.semanticscholar.org/author/Ben-Feinstein/4478112">Ben Finestein*</a>,
				<strong>Niv Haim*</strong>,
				<a href="https://www.weizmann.ac.il/math/irani/">Michal Irani</a>
              <br>
              <em>Technical Report</em>, 2020
              <br>
				<div class="btn-group">
				<div class="tooltip"><button>Abstract</button><span class="tooltiptext">A basic operation in Convolutional Neural Networks (CNNs) is spatial resizing of feature maps. This is done either by strided convolution (donwscaling) or transposed convolution (upscaling). Such operations are limited to a fixed filter moving at predetermined integer steps (strides). Spatial sizes of consecutive layers are related by integer scale factors, predetermined at architectural design, and remain fixed throughout training and inference time. We propose a generalization of the common Conv-layer, from a discrete layer to a Continuous Convolution (CC) Layer. CC Layers naturally extend Conv-layers by representing the filter as a learned continuous function over sub-pixel coordinates. This allows learnable and principled resizing of feature maps, to any size, dynamically and consistently across scales. Once trained, the CC layer can be used to output any scale/size chosen at inference time. The scale can be non-integer and differ between the axes. CC gives rise to new freedoms for architectural design, such as dynamic layer shapes at inference time, or gradual architectures where the size changes by a small factor at each layer. This gives rise to many desired CNN properties, new architectural design capabilities, and useful applications. We further show that current Conv-layers suffer from inherent misalignments, which are ameliorated by CC layers.</span></div>
				<a href="https://arxiv.org/abs/2006.11120"><button>ArXiv</button></a>
				<button onclick=copyToClipboard(bibtex_cc)>bibtex</button>
				</div>
              <p>Learning continuous convolution kernels improve translation equivariance and allow test time scales augmentations</p>
            </td>
		</tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/igr.png" alt="clean-usnob" width="160" id="igr_image">
			  <script type="text/javascript">
				var bibtex_igr = "@incollection{icml2020_2086,\n author = {Gropp, Amos and Yariv, Lior and Haim, Niv and Atzmon, Matan and Lipman, Yaron},\n booktitle = {Proceedings of Machine Learning and Systems 2020},\n pages = {3569--3579},\n title = {Implicit Geometric Regularization for Learning Shapes},\n year = {2020}\n}"
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2002.10099">
                <papertitle>Implicit Geometric Regularization for Learning Shapes</papertitle>
              </a>
              <br>
				<a href="https://scholar.google.com/citations?user=m70l1OEAAAAJ">Amos Gropp</a>,
				<a href="https://lioryariv.github.io/">Lior Yariv</a>,
				<strong>Niv Haim</strong>,
				<a href="https://matanatz.github.io/">Matan Atzmon</a>,
				<a href="http://www.wisdom.weizmann.ac.il/~ylipman/">Yaron Lipman</a>
              <br>
              <em>ICML</em>, 2020
			  <br>
				<div class="btn-group">
				<div class="tooltip"><button>Abstract</button><span class="tooltiptext">Representing shapes as level sets of neural networks has been recently proved to be useful for different shape analysis and reconstruction tasks. So far, such representations were computed using either: (i) pre-computed implicit shape representations; or (ii) loss functions explicitly defined over the neural level sets. In this paper we offer a new paradigm for computing high fidelity implicit neural representations directly from raw data (i.e., point clouds, with or without normal information). We observe that a rather simple loss function, encouraging the neural network to vanish on the input point cloud and to have a unit norm gradient, possesses an implicit geometric regularization property that favors smooth and natural zero level set surfaces, avoiding bad zero-loss solutions. We provide a theoretical analysis of this property for the linear case, and show that, in practice, our method leads to state of the art implicit neural representations with higher level-of-details and fidelity compared to previous methods.</span></div>
				<a href="https://arxiv.org/abs/2002.10099"><button>ArXiv</button></a>
				<a href="https://github.com/amosgropp/IGR"><button>GitHub</button></a>
				<a href="https://www.youtube.com/watch?v=6cOvBGBQF9g"><button>Video</button></a>
				<button onclick=copyToClipboard(bibtex_igr)>bibtex</button>
				</div>
              <p>Using an "Eikonal regularization" term with implicit neural representation works surprisingly well for modelling complex surfaces</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/controllinglevelsets.png" alt="clean-usnob" width="160" id="controlinglevelsets_image">
			  <script type="text/javascript">
				var bibtex_controllinglevelsets = "@inproceedings{atzmon2019controlling,\n  title={Controlling neural level sets},\n  author={Atzmon, Matan and Haim, Niv and Yariv, Lior and Israelov, Ofer and Maron, Haggai and Lipman, Yaron},\n  booktitle={Advances in Neural Information Processing Systems},\n  pages={2032--2041},\n  year={2019}\n}"
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1905.11911">
                <papertitle>Controlling Neural Level Sets</papertitle>
              </a>
              <br>
				<a href="https://matanatz.github.io/">Matan Atzmon</a>,
				<strong>Niv Haim</strong>,
				<a href="https://lioryariv.github.io/">Lior Yariv</a>,
				Ofer Israelov,
				<a href="https://haggaim.github.io/">Haggai Maron</a>,
				<a href="http://www.wisdom.weizmann.ac.il/~ylipman/">Yaron Lipman</a>
              <br>
              <em>NeurIPS</em>, 2019
			  <br>
				<div class="btn-group">
				<div class="tooltip"><button>Abstract</button><span class="tooltiptext">The level sets of neural networks represent fundamental properties such as decision boundaries of classifiers and are used to model non-linear manifold data such as curves and surfaces. Thus, methods for controlling the neural level sets could find many applications in machine learning. In this paper we present a simple and scalable approach to directly control level sets of a deep neural network. Our method consists of two parts: (i) sampling of the neural level sets, and (ii) relating the samples' positions to the network parameters. The latter is achieved by a sample network that is constructed by adding a single fixed linear layer to the original network. In turn, the sample network can be used to incorporate the level set samples into a loss function of interest. We have tested our method on three different learning tasks: improving generalization to unseen data, training networks robust to adversarial attacks, and curve and surface reconstruction from point clouds. For surface reconstruction, we produce high fidelity surfaces directly from raw 3D point clouds. When training small to medium networks to be robust to adversarial attacks we obtain robust accuracy comparable to state-of-the-art methods.</span></div>
				<a href="https://arxiv.org/abs/1905.11911"><button>ArXiv</button></a>
				<a href="https://github.com/matanatz/ControllingNeuralLevelsets"><button>GitHub</button></a>
				<a href="https://github.com/matanatz/ControllingNeuralLevelsets/blob/master/Controlling_Neural_Level_Sets_Poster.pdf"><button>Poster</button></a>
				<button onclick=copyToClipboard(bibtex_controllinglevelsets)>bibtex</button>
				</div>
              <p>Making input points differentiable (w.r.t model parameters), and using it for shape modelling, improved robustness to adversrial examples and more</p>
            </td>
          </tr>


		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/surfacenets.png" alt="clean-usnob" width="160" id="surfacenets_image">
			  <script type="text/javascript">
				var bibtex_surfacenets = "@inproceedings{haim2019surface,\n  title={Surface networks via general covers},\n  author={Haim, Niv and Segol, Nimrod and Ben-Hamu, Heli and Maron, Haggai and Lipman, Yaron},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={632--641},\n  year={2019}\n}"
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1812.10705">
                <papertitle>Surface Networks via General Covers</papertitle>
              </a>
              <br>
				<strong>Niv Haim*</strong>,
				<a href="https://scholar.google.com/citations?user=dTuhEVsAAAAJ&hl=en">Nimrod Segol*</a>,
				<a href="https://helibenhamu.github.io/">Heli Ben-Hamu</a>,
				<a href="https://haggaim.github.io/">Haggai Maron</a>,
				<a href="http://www.wisdom.weizmann.ac.il/~ylipman/">Yaron Lipman</a>
              <br>
              <em>ICCV</em>, 2019
			  <br>
				<div class="btn-group">
				<div class="tooltip"><button>Abstract</button><span class="tooltiptext">Developing deep learning techniques for geometric data is an active and fruitful research area. This paper tackles the problem of sphere-type surface learning by developing a novel surface-to-image representation. Using this representation we are able to quickly adapt successful CNN models to the surface setting. The surface-image representation is based on a covering map from the image domain to the surface. Namely, the map wraps around the surface several times, making sure that every part of the surface is well represented in the image. Differently from previous surface-to-image representations, we provide a low distortion coverage of all surface parts in a single image. Specifically, for the use case of learning spherical signals, our representation provides a low distortion alternative to several popular spherical parameterizations used in deep learning. We have used the surface-to-image representation to apply standard CNN architectures to 3D models as well as spherical signals. We show that our method achieves state of the art or comparable results on the tasks of shape retrieval, shape classification and semantic shape segmentation.</span></div>
				<a href="https://arxiv.org/abs/1812.10705"><button>Paper</button></a>
				<a href="https://github.com/nivha/surface_networks_covers"><button>GitHub</button></a>				
				<button onclick=copyToClipboard(bibtex_surfacenets)>bibtex</button>
				</div>
              <p>Transforming 3D shapes to image representation so we can feed them to off-the-shelf CNNs and do classification, human-parts segmentation and more</p>
            </td>
          </tr>


		<!-- <tr onmouseout="extreme_approaches_stop()" onmouseover="extreme_approaches_start()"> -->
		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='extreme_approaches_vid'>
					<video  width=100% height=100% muted autoplay loop>
						<source src="images/extreme_approaches.mp4" style="background-color: white;" type="video/mp4">
						Your browser does not support the video tag.
					</video>
				</div>
                <!-- <img src='images/extreme_approaches.png' width="160" id="extreme_approaches_image"> -->
              </div>
              <script type="text/javascript">
			  var bibtex_extreme_approaches = "@article{haim2018extreme,\n  title={Extreme close approaches in hierarchical triple systems with comparable masses},\n  author={Haim, Niv and Katz, Boaz},\n  journal={Monthly Notices of the Royal Astronomical Society},\n  volume={479},\n  number={3},\n  pages={3155--3166},\n  year={2018},\n  publisher={Oxford University Press}\n}"
                <!-- function extreme_approaches_start() { -->
                  <!-- document.getElementById('extreme_approaches_image').style.display = 'inline'; -->
				  <!-- document.getElementById('extreme_approaches_vid').style.display = 'none'; -->
                <!-- } -->
                <!-- function extreme_approaches_stop() { -->
                  <!-- document.getElementById('extreme_approaches_image').style.display = 'none'; -->
				  <!-- document.getElementById('extreme_approaches_vid').style.display = 'inline'; -->
                <!-- } -->
                <!-- extreme_approaches_stop() -->
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1803.10249">
              <papertitle>Extreme close approaches in hierarchical triple systems with comparable masses</papertitle>
              </a>
              <br>
				<strong>Niv Haim</strong>,
				<a href="https://www.weizmann.ac.il/particle/katz/">Boaz Katz</a>
              <br>
              <em>Monthly Notices of the Royal Astronomical Society</em>, 2018
              <br>
				<div class="btn-group">
				<div class="tooltip"><button>Abstract</button><span class="tooltiptext">We study close approaches in hierarchical triple systems with comparable masses using full N-body simulations, motivated by a recent model for type Ia supernovae involving direct collisions of white dwarfs (WDs). For stable hierarchical systems where the inner binary components have equal masses, we show that the ability of the inner binary to achieve very close approaches, where the separation between the components of the inner binary reaches values which are orders of magnitude smaller than the semi-major axis, can be analytically predicted from initial conditions. The rate of close approaches is found to be roughly linear with the mass of the tertiary. The rate increases in systems with unequal inner binaries by a marginal factor of â‰²2 for mass ratios 0.5 &#60; m1/m2 &#62; 1 relevant for the inner white-dwarf binaries. For an average tertiary mass of ~0.3M_sun which is representative of typical M-dwarfs, the chance for clean collisions is ~1% setting challenging constraints on the collisional model for type Ia's.</span></div>
				<a href="https://arxiv.org/abs/1803.10249"><button>ArXiv</button></a>
				<a href="https://github.com/nivha/three_body_integration"><button>GitHub</button></a>
				<button onclick=copyToClipboard(bibtex_extreme_approaches)>bibtex</button>
				</div>
              <p>Ever wondered if your hierarchical three-body system will eventually collide? find out by plugging your initial conditions into our analytical prediction formula (that works with high probability)</p>
            </td>
		</tr>

        </tbody></table>


		<hr/>
		
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
		  <tr>
			<td style="width:25%;vertical-align:middle;text-align: center;"><img src="images/weizmann.jpg"><span style="display: inline-block; font-weight: bold;">Weizmann Institute <br/> of Science </span></td>
            <td width="75%" valign="center">
              Advanced Topics in Computer Vision and Deep Learning, Spring 2021 [<a href="https://weizmannvision.github.io/adlv2021/">website</a>]
              <br/><br/>
              Introduction to Adversarial Examples, guest tutorial in DL4CV, Spring 2021 [<a href="https://www.youtube.com/watch?v=tpcYM0JOZ4s&list=PL_Z2_U9MIJdNgFM7-f2fZ9ZxjVRP_jhJv">YouTube</a>]
              <br/><br/>
              Advanced Topics in Computer Vision and Deep Learning, Spring 2020 [<a href="http://www.wisdom.weizmann.ac.il/~/vision/courses/2020_2/ADLV/index.html">website</a>]
			  <br/><br/>
              Deep Neural Networks - a Hands-On Challenge, Spring 2017 [<a href="http://www.wisdom.weizmann.ac.il/~vision/courses/2017_2/DNN%20Challenge/index.html">website</a>]
			  <br/><br/>
			  <a href="https://nivhaim.medium.com/how-to-give-a-good-students-seminar-presentation-2c060650ba38">Blog Post: How to Give a Good Student Seminar Presentation </a>
            </td>
          </tr>
        </tbody></table>


		<hr/>
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Miscellaneous</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
		  <tr>
            <td width="100%" valign="center">
              I play the violin [<a href="https://www.youtube.com/watch?v=Lnv-VTFBrA8">YouTube</a>]
              <br/><br/>
			  I sometimes write about my travels [<a href="https://mectype.wordpress.com/">blog</a>]
              <br/><br/>
            </td>
          </tr>
        </tbody></table>



		<hr/>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="margin-top:-20px;text-align:center;font-size:small;">
                This website is based on <a href="https://jonbarron.info/">Jon Barron</a>'s template (<a href="https://github.com/jonbarron/jonbarron_website">source code</a>)
              </p>
            </td>
          </tr>
        </tbody></table>

		
      </td>
    </tr>
  </table>
</body>

</html>
